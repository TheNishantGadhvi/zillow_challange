{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Kaggle Submission\n",
    "According to the Kaggle site, our submission file needs to look like this:\n",
    "\n",
    "ParcelId,201610,201611,201612,201710,201711,201712\n",
    "10754147,0.1234,1.2234,-1.3012,1.4012,0.8642-3.1412\n",
    "10759547,0,0,0,0,0,0\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './helper')\n",
    "from helpers import read_in_dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "CHUNK_SIZE = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the model we created in 05_FitFinalModel.ipynb to make our submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = joblib.load('models/model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Naive Models for Comparison\n",
    "Let's also create a couple naive \"models\" to compare to ours. Since we want to minimize Mean Abolute Error, the median of the target in the train dataset will be a good naive model to try (the median actually does minimize the mean absolute error of a dataset if all you can predict is a constant). Another good one to try would be predicting all 0's as we know this dataset is model residuals (which should be zero), so predicting zero is essentially accepting that the Zillow model can get no better.\n",
    "\n",
    "Any model that gives worse performance than either of these two naive models is totally, utterly, USELESS!! I've seen Kaggle submissions that don't hit this threshold, which is totally bonkers! This is a great lesson in why it's important to remember why you're doing what you're doing instead of concentrating on minimizing some loss function or spending a bunch of time and energy and computational resources tuning hyperparameters when something is obviously wrong with your setup (i.e. you're not beating the naive models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class medianPredictor:\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.med = X.median()\n",
    "        \n",
    "    def predict(self, X, y=None):\n",
    "        return np.array([self.med] * len(X))\n",
    "    \n",
    "class zeroPredictor:\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        return np.array([0] * len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are helper functions to help generate the submission file. Since we're asked to make 6 predictions on all ~3 million properties, we need to make ~18 million predictions. Instead of creating a giant dataframe that is 18 million rows long, I'll simply make roughly 50k predictions at a time to avoid using up all the memory on my machine. Since we wrote our code not to depend on the size of the training data, this will have no effect on our final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(df, chunksize):\n",
    "    \"\"\"Generator to return chunks of a dataframe of a given size\"\"\"\n",
    "    chunk = 1\n",
    "    total = len(df) // chunksize + 1\n",
    "    while chunk <= total:\n",
    "        if chunk < total:\n",
    "            yield df.iloc[((chunk-1)*chunksize) : (chunk*chunksize)]\n",
    "        else:\n",
    "            yield df.iloc[(chunk-1)*chunksize:]\n",
    "        chunk += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date(df, dt):\n",
    "    df['transactiondate'] = pd.to_datetime(dt)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code to actually make the submission file. Basically it loops through the chunks of the 2016 properties dataset and the dates we're asked to predict on and uses a model (be it the median, zero, or gbm model) to generate the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sub_file(model, chunksize):\n",
    "    \n",
    "    dates = ['2016-10-01', '2016-11-01', '2016-12-01', '2017-10-01', '2017-11-01', '2017-12-01']\n",
    "    props = read_in_dataset('properties_2016')\n",
    "    \n",
    "    submission_df = pd.DataFrame(index=props.parcelid)\n",
    "    \n",
    "    for d in dates:\n",
    "        props = add_date(props, d)\n",
    "        for x in make_chunks(props, chunksize):\n",
    "            preds = model.predict(x)\n",
    "            ix = x.parcelid\n",
    "            submission_df.loc[ix,str(pd.to_datetime(d).year) + str(pd.to_datetime(d).month)] = preds\n",
    "        print('Processed date {0}'.format(d))\n",
    "    \n",
    "    del props\n",
    "    \n",
    "    return submission_df.round(4).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a directory to hold the submissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just predict the Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed date 2016-10-01\n",
      "Processed date 2016-11-01\n",
      "Processed date 2016-12-01\n",
      "Processed date 2017-10-01\n",
      "Processed date 2017-11-01\n",
      "Processed date 2017-12-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35755"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = medianPredictor()\n",
    "mp.fit(read_in_dataset('train_2016').logerror)\n",
    "make_sub_file(mp, CHUNK_SIZE).to_csv('submissions/median_submission.csv', index=False)\n",
    "gc.collect() # because of memory issues, garbage collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just predict 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nishant/Documents/DSDJ/data-science-conda/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3296: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed date 2016-10-01\n",
      "Processed date 2016-11-01\n",
      "Processed date 2016-12-01\n",
      "Processed date 2017-10-01\n",
      "Processed date 2017-11-01\n",
      "Processed date 2017-12-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zp = zeroPredictor()\n",
    "# don't need to fit\n",
    "make_sub_file(zp, CHUNK_SIZE).to_csv('submissions/zero_submission.csv', index=False)\n",
    "gc.collect() # because of memory issues, garbage collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed date 2016-10-01\n",
      "Processed date 2016-11-01\n",
      "Processed date 2016-12-01\n",
      "Processed date 2017-10-01\n",
      "Processed date 2017-11-01\n",
      "Processed date 2017-12-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_sub_file(my_model, CHUNK_SIZE).to_csv('submissions/model_submission.csv', index=False)\n",
    "gc.collect() # because of memory issues, garbage collect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
